import torch
import torch.nn as nn
from torch.nn import functional as F


class EnhancedFusionModel(nn.Module):
    """å¢å¼ºç‰ˆå¤šæ¨¡æ€èåˆæ¨¡å‹

    æ”¯æŒå¤šç§èåˆç­–ç•¥ï¼šconcat, attention_fusion, cross_attention, tensor_fusion
    æ”¯æŒ ArcFace æŸå¤±å‡½æ•°ï¼ˆä¸å•æ¨¡æ€è®­ç»ƒå¯¹é½ï¼‰
    """

    def __init__(self, face_embedding_dim=512, fingerprint_embedding_dim=256,
                 num_classes=100, hidden_dim=512, dropout_rate=0.5,
                 fusion_method='concat'):
        super(EnhancedFusionModel, self).__init__()

        self.face_embedding_dim = face_embedding_dim
        self.fingerprint_embedding_dim = fingerprint_embedding_dim
        self.num_classes = num_classes
        self.fusion_method = fusion_method

        if fusion_method == 'concat':
            # ç®€å•æ‹¼æ¥ + MLP
            fused_dim = face_embedding_dim + fingerprint_embedding_dim
            # LayerNorm ç¨³å®šæ•°å€¼åˆ†å¸ƒ
            self.fusion_norm = nn.LayerNorm(fused_dim)
            self.fusion_layers = nn.Sequential(
                nn.Linear(fused_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            # ç‰¹å¾è¾“å‡ºå±‚ï¼ˆç”¨äºArcFaceï¼‰
            self.feature_layer = nn.Linear(hidden_dim, hidden_dim)
            self.feature_norm = nn.LayerNorm(hidden_dim)
            # å ä½ç¬¦åˆ†ç±»å™¨ï¼ˆç”¨äºåŠ è½½æ£€æŸ¥ç‚¹ï¼‰
            self._classifier = None

        elif fusion_method == 'attention_fusion':
            # LayerNorm ç¨³å®šæ•°å€¼åˆ†å¸ƒ
            self.fusion_norm = nn.LayerNorm(face_embedding_dim + fingerprint_embedding_dim)
            # æ³¨æ„åŠ›èåˆ - åˆ†åˆ«è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„æ³¨æ„åŠ›æƒé‡
            self.face_attention = nn.Sequential(
                nn.Linear(face_embedding_dim, face_embedding_dim // 4),
                nn.ReLU(inplace=True),
                nn.Linear(face_embedding_dim // 4, 1),
                nn.Sigmoid()
            )

            self.fp_attention = nn.Sequential(
                nn.Linear(fingerprint_embedding_dim, fingerprint_embedding_dim // 4),
                nn.ReLU(inplace=True),
                nn.Linear(fingerprint_embedding_dim // 4, 1),
                nn.Sigmoid()
            )

            self.fusion_layers = nn.Sequential(
                nn.Linear(face_embedding_dim + fingerprint_embedding_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            self.feature_layer = nn.Linear(hidden_dim, hidden_dim)
            self.feature_norm = nn.LayerNorm(hidden_dim)
            self._classifier = None

        elif fusion_method == 'cross_attention':
            # LayerNorm ç¨³å®šæ•°å€¼åˆ†å¸ƒ
            self.fusion_norm = nn.LayerNorm(face_embedding_dim * 2)
            # è·¨æ¨¡æ€æ³¨æ„åŠ› - å…ˆæŠ•å½±åˆ°ç›¸åŒç»´åº¦
            self.cross_proj = nn.Linear(fingerprint_embedding_dim, face_embedding_dim)
            self.cross_attention = CrossModalAttention(
                embed_dim=face_embedding_dim,
                num_heads=8
            )

            self.fusion_layers = nn.Sequential(
                nn.Linear(face_embedding_dim * 2, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            self.feature_layer = nn.Linear(hidden_dim, hidden_dim)
            self.feature_norm = nn.LayerNorm(hidden_dim)
            self._classifier = None

        elif fusion_method == 'tensor_fusion':
            # å¼ é‡èåˆ (æ›´å¤æ‚çš„äº¤äº’) - å·²æœ‰å†…ç½®norm
            self.tensor_fusion = TensorFusion(
                face_dim=face_embedding_dim,
                fingerprint_dim=fingerprint_embedding_dim,
                hidden_dim=hidden_dim
            )

            self.fusion_layers = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.BatchNorm1d(hidden_dim // 2),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            self.feature_layer = nn.Linear(hidden_dim // 2, hidden_dim)
            self.feature_norm = nn.LayerNorm(hidden_dim)
            self._classifier = None
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆæ–¹æ³•: {fusion_method}")

        # ArcFace åˆ†ç±»å™¨å ä½ç¬¦ï¼ˆå¤–éƒ¨è®¾ç½®ï¼‰
        self.arc_classifier = None

        self._initialize_weights()

    @property
    def classifier(self):
        """è·å–åˆ†ç±»å™¨ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰"""
        return self._classifier

    @classifier.setter
    def classifier(self, value):
        """è®¾ç½®åˆ†ç±»å™¨"""
        self._classifier = value

    def _extract_fused_features(self, face_features, fingerprint_features):
        """æå–èåˆç‰¹å¾ï¼ˆä¸å«åˆ†ç±»ï¼‰"""
        # L2å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾
        face_norm = F.normalize(face_features, p=2, dim=1)
        fp_norm = F.normalize(fingerprint_features, p=2, dim=1)

        if self.fusion_method == 'concat':
            fused = torch.cat([face_norm, fp_norm], dim=1)
            fused = self.fusion_norm(fused)
            fused = self.fusion_layers(fused)

        elif self.fusion_method == 'attention_fusion':
            face_weight = self.face_attention(face_norm)
            fp_weight = self.fp_attention(fp_norm)
            attended_face = face_norm * face_weight
            attended_fp = fp_norm * fp_weight
            fused = torch.cat([attended_face, attended_fp], dim=1)
            fused = self.fusion_norm(fused)
            fused = self.fusion_layers(fused)

        elif self.fusion_method == 'cross_attention':
            projected_fp = self.cross_proj(fp_norm)
            attended = self.cross_attention(face_norm, projected_fp)
            fused = torch.cat([face_norm, attended], dim=1)
            fused = self.fusion_norm(fused)
            fused = self.fusion_layers(fused)

        elif self.fusion_method == 'tensor_fusion':
            fused = self.tensor_fusion(face_norm, fp_norm)
            fused = self.fusion_norm(fused)
            fused = self.fusion_layers(fused)

        # è¾“å‡ºå½’ä¸€åŒ–ç‰¹å¾
        output_features = self.feature_norm(self.feature_layer(fused))
        output_features = F.normalize(output_features, p=2, dim=1)

        return output_features

    def forward(self, face_features, fingerprint_features, labels=None):
        """å‰å‘ä¼ æ’­

        Args:
            face_features: äººè„¸ç‰¹å¾ [batch, face_dim]
            fingerprint_features: æŒ‡çº¹ç‰¹å¾ [batch, fp_dim]
            labels: æ ‡ç­¾ï¼ˆç”¨äºArcFaceè®­ç»ƒï¼‰
        """
        fused_features = self._extract_fused_features(face_features, fingerprint_features)

        # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ArcFaceï¼ˆå¦‚æœè®¾ç½®ï¼‰
        if self.training and labels is not None and self.arc_classifier is not None:
            return self.arc_classifier(fused_features, labels)

        # è¯„ä¼°æ¨¡å¼æˆ–æ— ArcFaceï¼šè¿”å›ç‰¹å¾ï¼ˆç”¨äºéªŒè¯ï¼‰æˆ–ç›´æ¥è¿”å›åˆ†ç±»ç»“æœ
        if self.arc_classifier is not None:
            logits = self.arc_classifier(fused_features, labels) if labels is not None else self.arc_classifier(fused_features)
            if self.training:
                return logits
            return logits, fused_features

        # æ— ArcFaceï¼šä½¿ç”¨å†…ç½®åˆ†ç±»å™¨ï¼ˆè¿”å›logitsï¼‰
        return fused_features

    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, face_features, fingerprint_features):
        # ğŸ”§ ã€æŒ‡ä»¤Aã€‘åœ¨æ‹¼æ¥ååº”ç”¨ LayerNorm ç¨³å®šæ•°å€¼
        if self.fusion_method == 'concat':
            fused = torch.cat([face_features, fingerprint_features], dim=1)
            fused = self.fusion_norm(fused)
            return self.classifier(fused)

        elif self.fusion_method == 'attention_fusion':
            # åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›æƒé‡
            face_weight = self.face_attention(face_features)  # [batch, 1]
            fp_weight = self.fp_attention(fingerprint_features)  # [batch, 1]

            # åº”ç”¨æ³¨æ„åŠ›æƒé‡
            attended_face = face_features * face_weight
            attended_fp = fingerprint_features * fp_weight

            # æ‹¼æ¥ååº”ç”¨ LayerNorm
            combined = torch.cat([attended_face, attended_fp], dim=1)
            combined = self.fusion_norm(combined)
            return self.classifier(combined)

        elif self.fusion_method == 'cross_attention':
            # å…ˆå°†æŒ‡çº¹ç‰¹å¾æŠ•å½±åˆ°ç›¸åŒç»´åº¦
            fp_proj = self.cross_proj(fingerprint_features)
            # äººè„¸ç‰¹å¾ä½œä¸ºqueryï¼ŒæŒ‡çº¹ç‰¹å¾ä½œä¸ºkey/value
            attended_features = self.cross_attention(face_features, fp_proj)
            combined = torch.cat([face_features, attended_features], dim=1)
            combined = self.fusion_norm(combined)
            return self.classifier(combined)

        elif self.fusion_method == 'tensor_fusion':
            fused = self.tensor_fusion(face_features, fingerprint_features)
            return self.classifier(fused)

    def extract_features(self, face_features, fingerprint_features):
        """æå–èåˆç‰¹å¾ï¼ˆç”¨äºç‰¹å¾åˆ†æï¼‰"""
        with torch.no_grad():
            if self.fusion_method == 'concat':
                fused = torch.cat([face_features, fingerprint_features], dim=1)
                # è¿”å›ç¬¬ä¸€å±‚ç‰¹å¾
                x = self.classifier[0](fused)
                x = self.classifier[1](x)  # BatchNorm
                x = self.classifier[2](x)  # ReLU
                return x

            elif self.fusion_method == 'attention_fusion':
                # åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›æƒé‡
                face_weight = self.face_attention(face_features)
                fp_weight = self.fp_attention(fingerprint_features)

                # åº”ç”¨æ³¨æ„åŠ›æƒé‡
                attended_face = face_features * face_weight
                attended_fp = fingerprint_features * fp_weight

                # æ‹¼æ¥ååˆ†ç±»
                combined = torch.cat([attended_face, attended_fp], dim=1)
                x = self.classifier[0](combined)
                x = self.classifier[1](x)
                x = self.classifier[2](x)
                return x

            elif self.fusion_method == 'cross_attention':
                fp_proj = self.cross_proj(fingerprint_features)
                attended_features = self.cross_attention(face_features, fp_proj)
                combined = torch.cat([face_features, attended_features], dim=1)
                x = self.classifier[0](combined)
                x = self.classifier[1](x)
                x = self.classifier[2](x)
                return x

            elif self.fusion_method == 'tensor_fusion':
                fused = self.tensor_fusion(face_features, fingerprint_features)
                x = self.classifier[0](fused)
                x = self.classifier[1](x)
                x = self.classifier[2](x)
                return x


class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, embed_dim, num_heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, query, key_value):
        # query: face_features, key_value: fingerprint_features
        # éœ€è¦æ‰©å±•ç»´åº¦ä»¥é€‚åº”å¤šå¤´æ³¨æ„åŠ›
        query = query.unsqueeze(1)  # [batch, 1, dim]
        key_value = key_value.unsqueeze(1)  # [batch, 1, dim]

        attended, _ = self.attention(query, key_value, key_value)
        attended = attended.squeeze(1)  # [batch, dim]

        return self.norm(attended + query.squeeze(1))


class TensorFusion(nn.Module):
    """å¼ é‡èåˆæ¨¡å— - å­¦ä¹ æ¨¡æ€é—´çš„å¤æ‚äº¤äº’"""

    def __init__(self, face_dim, fingerprint_dim, hidden_dim):
        super().__init__()

        # å¤–ç§¯å¼ é‡èåˆ
        self.tensor_weight = nn.Parameter(torch.randn(face_dim, fingerprint_dim, hidden_dim))
        self.bias = nn.Parameter(torch.zeros(hidden_dim))

        # æ®‹å·®è¿æ¥
        self.residual_proj = nn.Linear(face_dim + fingerprint_dim, hidden_dim)

        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, face_feat, fingerprint_feat):
        # å¼ é‡èåˆ: face_feat âŠ— fingerprint_feat
        # [batch, face_dim] -> [batch, face_dim, 1]
        face_expanded = face_feat.unsqueeze(2)
        # [batch, fingerprint_dim] -> [batch, 1, fingerprint_dim]
        fp_expanded = fingerprint_feat.unsqueeze(1)

        # å¤–ç§¯: [batch, face_dim, fingerprint_dim]
        outer_product = torch.matmul(face_expanded, fp_expanded)

        # ä¸å¼ é‡æƒé‡ç›¸ä¹˜: [batch, face_dim, fingerprint_dim] @ [face_dim, fingerprint_dim, hidden_dim]
        # -> [batch, hidden_dim]
        fused = torch.einsum('bij,ijk->bk', outer_product, self.tensor_weight)

        # æ®‹å·®è¿æ¥
        residual = self.residual_proj(torch.cat([face_feat, fingerprint_feat], dim=1))

        output = fused + residual + self.bias
        return self.norm(self.dropout(output))


# ä¿æŒå‘åå…¼å®¹æ€§
class FusionModel(EnhancedFusionModel):
    """å‘åå…¼å®¹çš„ç®€å•èåˆæ¨¡å‹"""

    def __init__(self, face_embedding_dim=512, fingerprint_embedding_dim=512,
                 num_classes=100, hidden_dim=256, dropout_rate=0.5):
        super().__init__(
            face_embedding_dim=face_embedding_dim,
            fingerprint_embedding_dim=fingerprint_embedding_dim,
            num_classes=num_classes,
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate,
            fusion_method='concat'
        )


def create_fusion_model(fusion_method='concat', **kwargs):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºèåˆæ¨¡å‹"""
    return EnhancedFusionModel(fusion_method=fusion_method, **kwargs)


def create_enhanced_fusion_model(**kwargs):
    """åˆ›å»ºå¢å¼ºç‰ˆèåˆæ¨¡å‹ï¼ˆåˆ«åï¼‰"""
    return EnhancedFusionModel(**kwargs)


# ============================================================
# Fusion Utilities - Feature Alignment for Face-Fingerprint Fusion
# ============================================================

def get_face_embedding_dim():
    """Get standard face embedding dimension."""
    return 512


def get_fingerprint_embedding_dim():
    """Get standard fingerprint embedding dimension."""
    return 256


def align_embedding_dims(face_features, fingerprint_features):
    """Align face (512D) and fingerprint (256D) embeddings for fusion.

    Projects face features from 512D to 256D using a linear projection.

    Args:
        face_features: Face embeddings [B, 512]
        fingerprint_features: Fingerprint embeddings [B, 256]

    Returns:
        Tuple of aligned embeddings (both 256D)
    """
    projection = nn.Linear(512, 256)
    face_aligned = projection(face_features)
    face_aligned = F.normalize(face_aligned, p=2, dim=1)
    return face_aligned, fingerprint_features